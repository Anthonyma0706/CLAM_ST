{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import time\n",
    "from datasets.dataset_h5 import Dataset_All_Bags, Whole_Slide_Bag\n",
    "from torch.utils.data import DataLoader\n",
    "from models.resnet_custom import resnet50_baseline\n",
    "import argparse\n",
    "from utils.utils import print_network, collate_features\n",
    "from utils.file_utils import save_hdf5\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Get all files in the directory\n",
    "folder = './test_10x_patches'\n",
    "img_ext = '.png'\n",
    "files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and f.endswith(img_ext)]\n",
    "print(len(files))\n",
    "\n",
    "# Read all images and append to a list\n",
    "images = []\n",
    "for file in files:\n",
    "    img_path = os.path.join(folder, file)\n",
    "    img = np.array(Image.open(img_path)) #cv2.imread(os.path.join(folder, file))\n",
    "    if img is not None:\n",
    "        images.append(img)\n",
    "    else:\n",
    "        print('WARNING: IMAGE NOT EXIST')\n",
    "\n",
    "# Convert list to numpy array\n",
    "images_np = np.array(images)\n",
    "\n",
    "h5_folder = f'{folder}/h5'\n",
    "# create csv folder if it doesn't exist\n",
    "if not os.path.isdir(h5_folder):\n",
    "    os.mkdir(h5_folder)\n",
    "else: # remove all files in csv folder\n",
    "    for f in os.listdir(h5_folder):\n",
    "        os.remove(os.path.join(h5_folder, f))\n",
    "\n",
    "# Create a new HDF5 file\n",
    "h5f = h5py.File(os.path.join(folder, 'image_data.h5'), 'w')\n",
    "\n",
    "# Store the images in the 'imgs' dataset\n",
    "h5f.create_dataset('imgs', data=images_np)\n",
    "\n",
    "# Close the file\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'test_10x_patches'\n",
    "files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "csv_folder = f'{folder}/csv'\n",
    "# create csv folder if it doesn't exist\n",
    "if not os.path.isdir(csv_folder):\n",
    "    os.mkdir(csv_folder)\n",
    "else: # remove all files in csv folder\n",
    "    for f in os.listdir(csv_folder):\n",
    "        os.remove(os.path.join(csv_folder, f))\n",
    "\n",
    "# Generate full paths\n",
    "full_paths = [f.split('.')[0] for f in files]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(full_paths, columns=['slide_id'])\n",
    "df\n",
    "# Save to CSV file\n",
    "df.to_csv(os.path.join(csv_folder, 'slide_id.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_10x_patches/slide_id.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(folder, 'slide_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_h5 import Dataset_All_Bags, Whole_Slide_Bag, Whole_Slide_Bag_FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './test_10x_patches/patch_0.jpg'\n",
    "img = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 201, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './test_10x_patches/h5/image_data.h5'\n",
    "pretrained = True\n",
    "custom_downsample=1\n",
    "target_patch_size= 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n",
      "transformations: <torchvision.transforms.Compose object at 0x7f69817dd7d0>\n",
      "target_size:  (224, 224)\n"
     ]
    }
   ],
   "source": [
    "dataset = Whole_Slide_Bag(file_path=file_path, pretrained=True, \n",
    "\t\t\t\t\t\t\t  target_patch_size=target_patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object 'coords' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-55a9d71005c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/anthony/projects/ST_codebase/CLAM_ST/datasets/dataset_h5.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imgs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                         \u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clam/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'coords' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w_loader(file_path, output_path, model, batch_size = 8, verbose = 0, \n",
    "\t  \t\t\t\t print_every=20, pretrained=True, target_patch_size=-1):\n",
    "\t\"\"\"\n",
    "\targs:\n",
    "\t\tfile_path: directory of bag (.h5 file)\n",
    "\t\toutput_path: directory to save computed features (.h5 file)\n",
    "\t\tmodel: pytorch model\n",
    "\t\tbatch_size: batch_size for computing features in batches\n",
    "\t\tverbose: level of feedback\n",
    "\t\tpretrained: use weights pretrained on imagenet\n",
    "\t\"\"\"\n",
    "\tdataset = Whole_Slide_Bag(file_path=file_path, pretrained=pretrained, \n",
    "\t\t\t\t\t\t\t  target_patch_size=target_patch_size)\n",
    "\t# x, y = dataset[0]\n",
    "\tkwargs = {'num_workers': 4, 'pin_memory': True} if device.type == \"cuda\" else {}\n",
    "\tloader = DataLoader(dataset=dataset, batch_size=batch_size, **kwargs, collate_fn=collate_features)\n",
    "\n",
    "\tif verbose > 0:\n",
    "\t\tprint('processing {}: total of {} batches'.format(file_path,len(loader)))\n",
    "\n",
    "\tmode = 'w'\n",
    "\tfor count, (batch, coords) in enumerate(loader):\n",
    "\t\twith torch.no_grad():\t\n",
    "\t\t\tif count % print_every == 0:\n",
    "\t\t\t\tprint('batch {}/{}, {} files processed'.format(count, len(loader), count * batch_size))\n",
    "\t\t\tbatch = batch.to(device, non_blocking=True)\n",
    "\t\t\tmini_bs = coords.shape[0]\n",
    "\t\t\t\n",
    "\t\t\tfeatures = model(batch)\n",
    "\t\t\t\n",
    "\t\t\tfeatures = features.cpu().numpy()\n",
    "\n",
    "\t\t\tasset_dict = {'features': features, 'coords': coords}\n",
    "\t\t\t\n",
    "\t\t\tsave_hdf5(output_path, asset_dict, attr_dict= None, mode=mode)\n",
    "\t\t\tmode = 'a'\n",
    "\t\n",
    "\treturn output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n"
     ]
    }
   ],
   "source": [
    "print('initializing dataset')\n",
    "data_dir = './RESULTS_test_prostate'\n",
    "csv_path = f'{data_dir}/csv/slide_id.csv' # args.csv_path\n",
    "feat_dir = './FEATURE_DIRECTORY'\n",
    "slide_ext = '.tif'\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "target_patch_size = 224\n",
    "bags_dataset = Dataset_All_Bags(csv_path)\n",
    "\n",
    "os.makedirs(feat_dir, exist_ok=True)\n",
    "dest_files = os.listdir(feat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./RESULTS_test_prostate/h5/prostate_adenocarcinoma_1.3.0.h5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(bags_dataset)\n",
    "slide_id = bags_dataset[0].split(slide_ext)[0]\n",
    "bag_name = slide_id + '.h5'\n",
    "os.path.join(data_dir, 'h5', f'{slide_id}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model checkpoint\n"
     ]
    }
   ],
   "source": [
    "print('loading model checkpoint')\n",
    "model = resnet50_baseline(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# print_network(model)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "model.eval()\n",
    "total = len(bags_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_path, clean_folder = True):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    else: # remove all files in folder\n",
    "        if clean_folder:\n",
    "            for f in os.listdir(folder_path):\n",
    "                os.remove(os.path.join(folder_path, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "progress: 0/1\n",
      "prostate_adenocarcinoma_1.3.0.h5\n",
      "pretrained: True\n",
      "transformations: <torchvision.transforms.Compose object at 0x7f4988f6a4d0>\n",
      "target_size:  (224, 224)\n",
      "processing ./RESULTS_test_prostate/h5/prostate_adenocarcinoma_1.3.0.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for ./FEATURE_DIRECTORY/h5_files/prostate_adenocarcinoma_1.3.0.h5 took 2.8208231925964355 s\n",
      "features size:  (4371, 1024)\n",
      "coordinates size:  (4371, 2)\n"
     ]
    }
   ],
   "source": [
    "for bag_candidate_idx in range(total):\n",
    "    slide_id = bags_dataset[bag_candidate_idx].split(slide_ext)[0]\n",
    "    bag_name = slide_id + '.h5'\n",
    "    bag_candidate = os.path.join(data_dir, 'h5', bag_name)\n",
    "    # bag_candidate = os.path.join(data_dir, 'h5', 'image_data.h5')\n",
    "\n",
    "    print('\\nprogress: {}/{}'.format(bag_candidate_idx, total))\n",
    "    print(bag_name)\n",
    "    # if not no_auto_skip and slide_id+'.pt' in dest_files:\n",
    "    #     print('skipped {}'.format(slide_id))\n",
    "    #     continue \n",
    "\n",
    "    output_folder = os.path.join(feat_dir, 'h5_files')\n",
    "    output_path = os.path.join(output_folder, bag_name)\n",
    "    \n",
    "    create_folder(output_folder, clean_folder = True)\n",
    "\n",
    "    pt_folder = os.path.join(feat_dir, 'pt_files')\n",
    "    create_folder(pt_folder, clean_folder = True)\n",
    "    \n",
    "    file_path = bag_candidate\n",
    "    time_start = time.time()\n",
    "    output_file_path = compute_w_loader(file_path, output_path, \n",
    "                                        model = model, batch_size = batch_size, \n",
    "                                        verbose = 1, print_every = 20,\n",
    "                                        target_patch_size=target_patch_size)\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print('\\ncomputing features for {} took {} s'.format(output_file_path, time_elapsed))\n",
    "    file = h5py.File(output_file_path, \"r\")\n",
    "\n",
    "    features = file['features'][:]\n",
    "    print('features size: ', features.shape)\n",
    "    print('coordinates size: ', file['coords'].shape)\n",
    "    features = torch.from_numpy(features)\n",
    "    bag_base, _ = os.path.splitext(bag_name)\n",
    "    torch.save(features, os.path.join(feat_dir, 'pt_files', bag_base+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'exist_ok' is an invalid keyword argument for mkdir()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8d73a9914760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h5_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'exist_ok' is an invalid keyword argument for mkdir()"
     ]
    }
   ],
   "source": [
    "os.mkdir(os.path.join(feat_dir, 'h5_files'), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './FEATURE_DIRECTORY/h5_files/prostate_adenocarcinoma_1.3.0.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c5280df6770b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh5f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./FEATURE_DIRECTORY/h5_files/prostate_adenocarcinoma_1.3.0.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/clam/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/clam/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './FEATURE_DIRECTORY/h5_files/prostate_adenocarcinoma_1.3.0.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "h5f = h5py.File('./FEATURE_DIRECTORY/h5_files/prostate_adenocarcinoma_1.3.0.h5', 'w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
