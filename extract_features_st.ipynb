{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import time\n",
    "from datasets.dataset_h5 import Dataset_All_Bags, Whole_Slide_Bag\n",
    "from torch.utils.data import DataLoader\n",
    "from models.resnet_custom import resnet50_baseline\n",
    "import argparse\n",
    "from utils.utils import print_network, collate_features\n",
    "from utils.file_utils import save_hdf5\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w_loader(file_path, output_path, model, batch_size = 8, verbose = 0, \n",
    "\t  \t\t\t\t print_every=20, pretrained=True, target_patch_size=-1):\n",
    "\t\"\"\"\n",
    "\targs:\n",
    "\t\tfile_path: directory of bag (.h5 file)\n",
    "\t\toutput_path: directory to save computed features (.h5 file)\n",
    "\t\tmodel: pytorch model\n",
    "\t\tbatch_size: batch_size for computing features in batches\n",
    "\t\tverbose: level of feedback\n",
    "\t\tpretrained: use weights pretrained on imagenet\n",
    "\t\"\"\"\n",
    "\tdataset = Whole_Slide_Bag(file_path=file_path, pretrained=pretrained, \n",
    "\t\t\t\t\t\t\t  target_patch_size=target_patch_size)\n",
    "\t# x, y = dataset[0]\n",
    "\tkwargs = {'num_workers': 4, 'pin_memory': True} if device.type == \"cuda\" else {}\n",
    "\tloader = DataLoader(dataset=dataset, batch_size=batch_size, **kwargs, collate_fn=collate_features)\n",
    "\n",
    "\tif verbose > 0:\n",
    "\t\tprint('processing {}: total of {} batches'.format(file_path,len(loader)))\n",
    "\n",
    "\tmode = 'w'\n",
    "\tfor count, (batch, coords) in enumerate(loader):\n",
    "\t\twith torch.no_grad():\t\n",
    "\t\t\tif count % print_every == 0:\n",
    "\t\t\t\tprint('batch {}/{}, {} files processed'.format(count, len(loader), count * batch_size))\n",
    "\t\t\tbatch = batch.to(device, non_blocking=True)\n",
    "\t\t\tmini_bs = coords.shape[0]\n",
    "\t\t\t\n",
    "\t\t\tfeatures = model(batch)\n",
    "\t\t\t\n",
    "\t\t\tfeatures = features.cpu().numpy()\n",
    "\n",
    "\t\t\tasset_dict = {'features': features, 'coords': coords}\n",
    "\t\t\t\n",
    "\t\t\tsave_hdf5(output_path, asset_dict, attr_dict= None, mode=mode)\n",
    "\t\t\tmode = 'a'\n",
    "\t\n",
    "\treturn output_path\n",
    "\n",
    "def create_folder(folder_path, clean_folder = True):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    else: # remove all files in folder\n",
    "        if clean_folder:\n",
    "            for f in os.listdir(folder_path):\n",
    "                os.remove(os.path.join(folder_path, f))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "RESULTS_DIRECTORY_ST/ (e.g RESULTS_test_prostate)   \n",
    "\t├── h5\n",
    "    \t\t├── prostate_adenocarcinoma_1.3.0.h5\n",
    "    \t\t└── ...\n",
    "\t├── csv\n",
    "    \t\t├── slide_id.csv\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "slide_id.csv:   \n",
    "slide_id   \n",
    "prostate_adenocarcinoma_1.3.0   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n",
      "loading model checkpoint\n"
     ]
    }
   ],
   "source": [
    "# SET UP FILE PATHS\n",
    "print('initializing dataset')\n",
    "data_dir = './RESULTS_test_prostate'\n",
    "csv_path = f'{data_dir}/csv/slide_id.csv'\n",
    "feat_dir = './FEATURE_DIRECTORY'\n",
    "slide_ext = '.tif'\n",
    "batch_size = 256\n",
    "target_patch_size = 224\n",
    "pretrained = True\n",
    "\n",
    "bags_dataset = Dataset_All_Bags(csv_path)\n",
    "total = len(bags_dataset)\n",
    "\n",
    "os.makedirs(feat_dir, exist_ok=True)\n",
    "dest_files = os.listdir(feat_dir)\n",
    "\n",
    "\n",
    "slide_id = bags_dataset[0].split(slide_ext)[0]\n",
    "bag_name = slide_id + '.h5'\n",
    "os.path.join(data_dir, 'h5', f'{slide_id}.h5')\n",
    "\n",
    "# LOAD MODEL\n",
    "print('loading model checkpoint')\n",
    "model = resnet50_baseline(pretrained=pretrained)\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "progress: 0/1\n",
      "prostate_adenocarcinoma_1.3.0.h5\n",
      "pretrained: True\n",
      "transformations: <torchvision.transforms.Compose object at 0x7fc9f1bbe7d0>\n",
      "target_size:  (224, 224)\n",
      "processing ./RESULTS_test_prostate/h5/prostate_adenocarcinoma_1.3.0.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for ./FEATURE_DIRECTORY/h5_files/prostate_adenocarcinoma_1.3.0.h5 took 6.6557183265686035 s\n",
      "features size:  (4371, 1024)\n",
      "coordinates size:  (4371, 2)\n"
     ]
    }
   ],
   "source": [
    "for bag_candidate_idx in range(total):\n",
    "    slide_id = bags_dataset[bag_candidate_idx].split(slide_ext)[0]\n",
    "    bag_name = slide_id + '.h5'\n",
    "    bag_candidate = os.path.join(data_dir, 'h5', bag_name)\n",
    "    \n",
    "    print('\\nprogress: {}/{}'.format(bag_candidate_idx, total))\n",
    "    print(bag_name)\n",
    "    # if not no_auto_skip and slide_id+'.pt' in dest_files:\n",
    "    #     print('skipped {}'.format(slide_id))\n",
    "    #     continue \n",
    "\n",
    "    output_folder = os.path.join(feat_dir, 'h5_files')\n",
    "    output_path = os.path.join(output_folder, bag_name)\n",
    "    \n",
    "    create_folder(output_folder, clean_folder = True)\n",
    "\n",
    "    pt_folder = os.path.join(feat_dir, 'pt_files')\n",
    "    create_folder(pt_folder, clean_folder = True)\n",
    "    \n",
    "    file_path = bag_candidate\n",
    "    time_start = time.time()\n",
    "    output_file_path = compute_w_loader(file_path, output_path, \n",
    "                                        model = model, batch_size = batch_size, \n",
    "                                        verbose = 1, print_every = 20,\n",
    "                                        target_patch_size=target_patch_size)\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print('\\ncomputing features for {} took {} s'.format(output_file_path, time_elapsed))\n",
    "    file = h5py.File(output_file_path, \"r\")\n",
    "\n",
    "    features = file['features'][:]\n",
    "    print('features size: ', features.shape)\n",
    "    print('coordinates size: ', file['coords'].shape)\n",
    "    features = torch.from_numpy(features)\n",
    "    bag_base, _ = os.path.splitext(bag_name)\n",
    "    torch.save(features, os.path.join(feat_dir, 'pt_files', bag_base+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
